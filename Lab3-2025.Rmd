---
title: 'P&S-2025: Lab assignment 3'
author: "Bohdan Zasymovych, Arsenii Stratiuk, "
output:
  html_document:
    df_print: paged
---

### Work breakdown:

-   Bohdan Zasymovych: Task 1

-   Arsenii Stratiuk: Task 3

```{r}
options(width = 500)
id <- 10
```

# Task 1

### **Note**:

**In all derivations quantiles are defined to have next property** $P(X < q_{\alpha}) = \alpha$

$$
\lambda = \frac{1}{\theta}
$$

## 1)

$$X_i \sim \mathcal{E}(\lambda)$$

### Statistic:

$$T(\textbf{X}) = 2 \lambda n \overline{\textbf{X}}$$

$$n \overline{\textbf{X}} = n \frac{\sum X_i}{n} = \sum X_i \sim \Gamma(n, \lambda)$$

$$W(\textbf{X}) = n \overline{\textbf{X}} \sim \Gamma(n, \lambda) $$

MGF for Gamma distribution ($\Gamma(n, \lambda)$):

$$M(t) = (\frac{\lambda}{\lambda-t})^n$$

MGF for Chi-squared distribution with $n$ degrees of freedom ($\chi^2_n$):

$$M(t) = (1-2t)^{-\frac{n}{2}}$$

MGF of $T$:

$$M_{2 \lambda W}(2 \lambda t) = (\frac{\lambda}{\lambda-2 \lambda t})^n = (1-2 t)^{-n}$$

So $T$ has Chi-squared distribution with $2n$ degrees of freedom ($T \sim \chi^2_{2n}$)

### Probability:

$$
\begin{align}
1 - \alpha &= P\left(x_{\alpha/2} \le T \le x_{1-\alpha/2}\right) =
P\left(x_{\alpha/2} \le 2 \alpha W \le x_{1-\alpha/2}\right) \newline
&= P\left(x_{\alpha/2} \le \frac{2W}{\theta} \le x_{1-\alpha/2}\right)
= P\left(\frac{1}{x_{1-\alpha/2}} \le \frac{\theta}{2W} \le \frac{1}{x_{\alpha/2}}\right) \newline
&= P\left(\frac{2W}{x_{1-\alpha/2}} \le \theta \le \frac{2W}{x_{\alpha/2}}\right)
= P\left(\frac{2n \overline{\textbf{X}}}{x_{1-\alpha/2}} \le \theta \le \frac{2n \overline{\textbf{X}}}{x_{\alpha/2}}\right)
\end{align}
$$

### Confidence interval of confidence level $1 - \alpha$:

$$ \left[ \frac{2n \overline{\textbf{X}}}{x_{1-\alpha/2}}, \quad \frac{2n \overline{\textbf{X}}}{x_{\alpha/2}} \right]
$$

Here:

$x_{\alpha}$ - $\alpha$ quantile of $\chi^2_{2n}$

## 2)

$$
\overline{\textbf{X}} \sim \mathcal{N}(\frac{1}{\lambda}, \frac{1}{n\lambda^2}) \implies \overline{\textbf{X}} \sim \mathcal{N}(\theta, \frac{\theta^2}{n}) 
$$

$$
P \left( \frac{\sqrt{n} |\overline{\textbf{X}} - \theta|}{\theta} \le z_\alpha \right) = 2 \alpha - 1
$$

$$ \begin{align}
2 \alpha - 1 &= P \left( \frac{\sqrt{n} |\theta - \overline{\textbf{X}}|}{\theta} \le z_\alpha \right) = P \left( |\theta - \overline{\textbf{X}}| \le  \frac{\theta}{\sqrt{n}} z_\alpha \right) \newline
&= P \left( \overline{\textbf{X}} - \frac{\theta}{\sqrt{n}} z_\alpha \le  \theta \le  \overline{\textbf{X}} +\frac{\theta}{\sqrt{n}} z_\alpha \right)
\end{align}
$$

Confidence interval of confidence level $2 \alpha - 1$:

$$
\left[ \overline{\textbf{X}} - \frac{\theta}{\sqrt{n}} z_\alpha, \; \overline{\textbf{X}} + \frac{\theta}{\sqrt{n}} z_\alpha \right]
$$

But bounds of this interval for $\theta$ is dependent of $\theta$ itself so it cannot be used in practice.

## 3)

$$
2 \alpha - 1 = P \left( \frac{\sqrt{n} |\theta - \overline{\textbf{X}}|}{\theta} \le z_\alpha \right) = P \left( |\theta - \overline{\textbf{X}}| \le  \frac{\theta}{\sqrt{n}} z_\alpha \right)
$$

To find interval for $\theta$ with bounds independent of a parameter we need to solve next inequality:

$$
|\theta - \overline{\textbf{X}}| \le  \frac{\theta}{\sqrt{n}} z_\alpha
$$

Upper bound for $\theta$:

$$
\theta - \frac{\theta}{\sqrt{n}} z_\alpha \le \overline{\textbf{X}} \\
\theta \left(1 - \frac{z_\alpha}{\sqrt{n}}\right) \le \overline{\textbf{X}} \\
\theta \le \frac{\overline{\textbf{X}}}{1 - z_\alpha/\sqrt{n}}
$$

Lower bound for $\theta$:

$$
\theta + \frac{\theta}{\sqrt{n}} z_\alpha \ge \overline{\textbf{X}} \\
\theta \left(1 + \frac{z_\alpha}{\sqrt{n}}\right) \ge \overline{\textbf{X}} \\
\theta \ge \frac{\overline{\textbf{X}}}{1 + z_\alpha/\sqrt{n}}
$$

Final inequality and probability:

$$\frac{\overline{\textbf{X}}}{1 + z_\alpha/\sqrt{n}} \le \theta \le \frac{\overline{\textbf{X}}}{1 - z_\alpha/\sqrt{n}}$$

$$
P \left( \frac{\overline{\textbf{X}}}{1 + z_\alpha/\sqrt{n}} \le \theta \le \frac{\overline{\textbf{X}}}{1 - z_\alpha/\sqrt{n}} \right) = 2 \alpha - 1
$$

Confidence interval of confidence level $2 \alpha - 1$: $$
\left[ \frac{\overline{\textbf{X}}}{1 + z_\alpha/\sqrt{n}}, \quad \frac{\overline{\textbf{X}}}{1 - z_\alpha/\sqrt{n}} \right]
$$

## 4)

$$
Sxx := \sum (X_i - \overline{\textbf{X}})^2
$$

Estimator for the variance of the sample mean $\overline{\textbf{X}}$:

$$
\frac{Sxx}{n(n-1)}
$$

Statistic:

$$
T(\textbf{X}) = \frac{\overline{\textbf{X}} - \theta}{\sqrt{\frac{Sxx}{n(n-1)}}} \sim \mathcal{T}_{n-1}
$$

Probability:

$$
\begin{align}
1 - \alpha &= P \left( t_{\alpha/2} \le \frac{\overline{\textbf{X}} - \theta}{\sqrt{\frac{Sxx}{n(n-1)}}} \le t_{1-\alpha / 2}\right) \newline
&= P \left( \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha/2} \le \overline{\textbf{X}} - \theta \le \sqrt{\frac{Sxx}{n(n-1)}} t_{1-\alpha / 2}\right) \newline
&= P \left( - \overline{\textbf{X}} + \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha/2} \le - \theta \le - \overline{\textbf{X}} + \sqrt{\frac{Sxx}{n(n-1)}} t_{1-\alpha / 2}\right) \newline
&= P \left( \overline{\textbf{X}} + \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha/2} \le \theta \le \overline{\textbf{X}} - \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha / 2}\right)
\end{align}
$$

Confidence interval for $\theta$ of level $1-\alpha$:

$$
\left[ \overline{\textbf{X}} + \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha/2}, \; \overline{\textbf{X}} - \sqrt{\frac{Sxx}{n(n-1)}} t_{\alpha / 2} \right]
$$

Here:

$t_\alpha$ - $\alpha$ quantile of t-Student distribution with $n-1$ degrees of freedom ($\mathcal{T_{n-1}}$)

## Simulations:

```{r}
run_simulation_trial <- function(n, confidence, theta, lambda, m_reps, ci_checker_func) {
  results_matrix <- matrix(0, nrow = m_reps, ncol = 2)
  
  for (i in 1:m_reps) {
    x <- rexp(n, rate = lambda)
    
    results_matrix[i, ] <- ci_checker_func(x, theta, confidence)
  }
  
  avg_length <- mean(results_matrix[, 1])
  est_coverage_prob <- mean(results_matrix[, 2])
  
  return(list(
    avg_length = round(avg_length, 4),
    est_coverage_prob = round(est_coverage_prob, 4)
  ))
}


run_all_simulations_by_m <- function(theta, n_values, m_values, confidences, ci_checker_func) {
  lambda <- 1 / theta
  
  all_m_results <- list()
  
  for (m_reps in m_values) {
    
    prob_table <- matrix(NA, nrow = length(n_values), ncol = length(confidences),
                         dimnames = list(paste0("n=", n_values), paste0("confidence=", confidences, "__")))
    length_table <- matrix(NA, nrow = length(n_values), ncol = length(confidences),
                           dimnames = list(paste0("n=", n_values), paste0("confidence=", confidences, "__")))
    
    for (i in 1:length(n_values)) {
      n <- n_values[i]
      
      for (j in 1:length(confidences)) {
        confidence <- confidences[j]
        
        trial_results <- run_simulation_trial(n, confidence, theta, lambda, m_reps, ci_checker_func)
      
        prob_table[i, j] <- trial_results$est_coverage_prob
        length_table[i, j] <- trial_results$avg_length
      }
    }
    
    all_m_results[[paste0("m=", m_reps)]] <- list(
      coverage_table = prob_table,
      length_table = length_table
    )
  }
  
  return(all_m_results)
}
```

```{r}
ci_check_chi2 <- function(sample_x, theta, confidence) {
  
  alpha = 1 - confidence
  x_bar <- mean(sample_x)
  n <- length(sample_x)
  
  q_low <- alpha / 2
  q_high <- 1 - alpha / 2
  
  chi_low_quant <- qchisq(q_low, df = 2 * n)
  chi_high_quant <- qchisq(q_high, df = 2 * n)
  
  ci_low <- 2 * n * x_bar / chi_high_quant
  ci_high <- 2 * n * x_bar / chi_low_quant
  
  contains_theta <- (ci_low <= theta) && (theta <= ci_high)
  ci_length <- ci_high - ci_low
  
  return(c(ci_length, as.numeric(contains_theta)))
}


ci_check_snormal_dependent <- function(sample_x, theta, confidence) {
  
  alpha = (confidence + 1)/2
  x_bar <- mean(sample_x)
  n <- length(sample_x)
  
  z_a_quant <- qnorm(alpha, mean=0, sd=1)
  
  ci_low <- x_bar - (theta/sqrt(n)) * z_a_quant
  ci_high <- x_bar + (theta/sqrt(n)) * z_a_quant
  
  contains_theta <- (ci_low <= theta) && (theta <= ci_high)
  ci_length <- ci_high - ci_low
  
  return(c(ci_length, as.numeric(contains_theta)))
}


ci_check_snormal <- function(sample_x, theta, confidence) {
  
  alpha = (confidence + 1)/2
  x_bar <- mean(sample_x)
  n <- length(sample_x)
  
  z_a_quant <- qnorm(alpha, mean=0, sd=1)
  
  ci_low <- x_bar / (1 + z_a_quant / sqrt(n))
  ci_high <- x_bar / (1 - z_a_quant / sqrt(n))
  
  contains_theta <- (ci_low <= theta) && (theta <= ci_high)
  ci_length <- ci_high - ci_low
  
  return(c(ci_length, as.numeric(contains_theta)))
}


ci_check_tstudent <- function(sample_x, theta, confidence) {
  
  alpha = 1 - confidence
  x_bar <- mean(sample_x)
  sample_sd <- sd(sample_x)
  n <- length(sample_x)
  
  tstud_quant <- qt(alpha/2, df = n-1)
  
  ci_low <- x_bar + (sample_sd / sqrt(n)) * tstud_quant
  ci_high <- x_bar - (sample_sd / sqrt(n)) * tstud_quant
  
  contains_theta <- (ci_low <= theta) && (theta <= ci_high)
  ci_length <- ci_high - ci_low
  
  return(c(ci_length, as.numeric(contains_theta)))
}
```

```{r}
theta <- id/10 # =1
n_values <- c(10, 100, 1000, 10000)
m_values <- c(10, 100, 1000)
confidences <- c(0.90, 0.95, 0.99)
```

```{r}
simulation_results_chi2 <- run_all_simulations_by_m(theta, n_values, m_values, confidences, ci_check_chi2)
simulation_results_snormal_dependent <- run_all_simulations_by_m(theta, n_values, m_values, confidences, ci_check_snormal_dependent)
simulation_results_snormal <- run_all_simulations_by_m(theta, n_values, m_values, confidences, ci_check_snormal)
simulation_results_tstudent <- run_all_simulations_by_m(theta, n_values, m_values, confidences, ci_check_tstudent)
```

### Estimated confidences of the intervals

#### CI using Chi-squared distributed statistic

```{r}
for (m_label in names(simulation_results_chi2)) {
  res <- simulation_results_chi2[[m_label]]
  
  print(as.data.frame(res$coverage_table))
}
```

#### CI using standart normal distributed statistic with bounds dependent on $\theta$

```{r}
for (m_label in names(simulation_results_snormal_dependent)) {
  res <- simulation_results_snormal_dependent[[m_label]]
  
  print(as.data.frame(res$coverage_table))
}
```

#### CI using standart normal distributed statistic

```{r}
for (m_label in names(simulation_results_snormal)) {
  res <- simulation_results_snormal[[m_label]]
  
  print(as.data.frame(res$coverage_table))
}
```

#### CI using t-student distributed statistic

```{r}
for (m_label in names(simulation_results_tstudent)) {
  res <- simulation_results_tstudent[[m_label]]
  
  print(as.data.frame(res$coverage_table))
}
```

### Lengths of the intervals

#### CI using Chi-squared distributed statistic

```{r}
for (m_label in names(simulation_results_chi2)) {
  res <- simulation_results_chi2[[m_label]]
  
  print(as.data.frame(res$length_table))
}
```

#### CI using standart normal distributed statistic with bounds dependent on $\theta$

```{r}
for (m_label in names(simulation_results_snormal_dependent)) {
  res <- simulation_results_snormal_dependent[[m_label]]
  
  print(as.data.frame(res$length_table))
}
```

#### CI using standart normal distributed statistic

```{r}
for (m_label in names(simulation_results_snormal)) {
  res <- simulation_results_snormal[[m_label]]
  
  print(as.data.frame(res$length_table))
}
```

#### CI using t-student distributed statistic

```{r}
for (m_label in names(simulation_results_tstudent)) {
  res <- simulation_results_tstudent[[m_label]]
  
  print(as.data.frame(res$length_table))
}
```

In the tests each dataframe correspond to different number of repetitions of the test ($m: 10,\, 100,\, 1000$)

For each number of repetitions 12 tests were conducted varying number of samples ($n: 10,\, 100,\, 100,\, 10000$) and confidence levels of intervals (confidence: $0.9,\, 0.95,\, 0.99$).

## Results

In this task, four methods of creating confidence intervals (CIs) for estimating the mean of exponentially distributed random variables were tested. Their empirical confidence levels and interval lengths were also examined. Tests were conducted with different numbers of repetitions ($m$), sample sizes ($n$), and confidence levels.

The first observation is that when $m$ increases, the estimated confidence levels become closer to the theoretical values due to the LLN. The same effect appears when $n$ increases.

For all methods, when $n$ and $m$ are large, the empirical confidence levels are close to the expected theoretical values, indicating that the CIs were derived correctly.

The interval which has bounds dependent on the parameter $\theta$ has the smallest length of all constructed intervals for all values of $n$ and all confidence levels. Also, it can be easily shown that the length of this interval does not depend on the observed data and is constant for a fixed $n$ and confidence level:

$$
\text{LENGTH} = \overline{\textbf{X}} + \frac{\theta}{\sqrt{n}} z_\alpha - \left( \overline{\textbf{X}} - \frac{\theta}{\sqrt{n}} z_\alpha \right) = \frac{2\theta}{\sqrt{n}} z_\alpha
$$

Tests are consistent with this fact: for all tested numbers of repetitions ($m$), all lengths are exactly the same. Since this interval does not depend on the observed data and uses both the known variance and known parameter $\theta$, it has the smallest possible length due to eliminated uncertainty. However, it is impractical. We used the parameter $\theta$ to create this interval which estimates $\theta$ itself. In practice, there is no sense in estimating a parameter if the exact value is known. That is why this method won't be included in the further analysis.

As $n$ increases, interval lengths decrease. This happens because the bounds of all three intervals include division by $n$, so increasing $n$ makes them converge toward the true parameter. In simple terms, larger samples give better parameter estimates. Intervals with higher confidence levels are longer because greater precision requires greater length.

For very small samples ($n=10$), CIs based on the t-student statistic produce the shortest intervals for all confidence levels. CIs based on the chi-squared statistic are slightly longer. The method using the statistic approximated by the standard normal distribution (via the CLT) produces much longer intervals, and the difference grows with higher confidence levels.

Although the estimated confidence levels are close to expectations, small deviations appear. For the CI using the chi-squared statistic, results almost match the theoretical value, since the statistic is distributed exactly as $\chi^2_{2n}$ without any approximation. For the other two methods, the normal approximation of the sample mean is imperfect. The high skewness of the exponential distribution leads to undercoverage, especially for small sample sizes.

In conclusion, if the distribution is not exactly exponential, only the methods using the standard normal or t-student statistics can be applied. If the variance is unknown, only the method using the t-student statistic is valid. For small sample sizes, if the variance is known and the distribution is exactly exponential, the best method is the chi-squaredâ€“based CI because it gives relatively short intervals without undercoverage. For small samples where the variance is unknown or the variables are not exactly exponentially distributed, the CI using the t-student statistic should be preferred. For large enough sample sizes ($>30$), all methods yield nearly the same results (given whether the variance is known and whether the distribution is exponential).

# Task 2

# Task 3

### **Problem Formulation**

We aim to compare two estimators for the population variance $\sigma^2$: the standard sample variance $\sigma^2_{n-1}$ (dividing by $n-1$) and the intuitive variance $\sigma^2_n$ (dividing by $n$). We will determine which one is "unbiased" (meaning its expected value equals the true parameter) by simulating samples from a Normal distribution with known variance $\sigma^2=4$ and calculating the bias for different sample sizes $n$.

### **R Code**

```{r}
# set seed for reproducibility (team id 10)
set.seed(10)

# parameters
mu <- 10
sigma_sq <- 4
m <- 10000  # number of repetitions to estimate expected value
ns <- c(10, 50, 100, 1000) # sample sizes

# vectors to store bias results
bias_n <- numeric(length(ns))
bias_n_minus_1 <- numeric(length(ns))

# setup for plotting
par(mfrow=c(2,2)) 

for (i in 1:length(ns)) {
  n <- ns[i]
  
  # generate m samples of size n (matrix with n rows, m cols)
  samples <- matrix(rnorm(n * m, mean = mu, sd = sqrt(sigma_sq)), nrow = n)
  
  # calculate standard unbiased variance (dividing by n-1)
  vars_unbiased <- apply(samples, 2, var)
  
  # calculate biased variance (dividing by n)
  # recover sum of squares by multiplying by (n-1), then divide by n
  vars_biased <- vars_unbiased * (n - 1) / n
  
  # calculate bias: mean of estimates minus true sigma_sq
  bias_n_minus_1[i] <- mean(vars_unbiased) - sigma_sq
  bias_n[i] <- mean(vars_biased) - sigma_sq
  
  # determine range to fit both histograms
  x_min <- min(c(vars_unbiased, vars_biased))
  x_max <- max(c(vars_unbiased, vars_biased))
  
  # plot unbiased (blue, transparent)
  hist(vars_unbiased, breaks=30, prob=TRUE, 
       col=rgb(0, 0, 1, 0.4), border="white",
       xlim=c(x_min, x_max),
       main=paste("Variance Estimators (n =", n, ")"), 
       xlab="Variance Estimate")
  
  # plot biased (red, transparent)
  hist(vars_biased, breaks=30, prob=TRUE, 
       col=rgb(1, 0, 0, 0.4), border="white", add=TRUE)
  
  # add vertical line for true variance
  abline(v = sigma_sq, col="black", lwd=2, lty=2)
  
  # add legend
  if (i == 1) { # Only add legend to the first plot to save space
    legend("topright", legend=c("Unbiased (n-1)", "Biased (n)", "True Sigma^2"), 
           fill=c(rgb(0,0,1,0.4), rgb(1,0,0,0.4), NA), 
           lty=c(NA, NA, 2), lwd=c(NA, NA, 2), border="white", bty="n", cex=0.8)
  }
}

# reset plotting layout
par(mfrow=c(1,1))

# output results table
results <- data.frame(
  sample_size = ns,
  bias_biased_n = bias_n,
  bias_unbiased_n_1 = bias_n_minus_1
)

print(results)
```

**Visual Analysis:**

The histograms visually demonstrate the concept of estimator bias:

**At $n=10$ (Top Left):** There is a clear separation. The **Red distribution (Biased)** is shifted to the left of the true value (Dashed Line at 4). This confirms that dividing by $n$ systematically underestimates the variance. The **Blue distribution (Unbiased)** is centered correctly on the dashed line.

**At $n=1000$ (Bottom Right):** The distributions overlap almost perfectly. This confirms that as sample size increases, the difference between dividing by $n$ and $n-1$ becomes negligible, and the bias vanishes.

### **Justification (Theoretical Derivation)**

**Derivation of Expectations (Task e):**

Let $SS = \sum_{i=1}^{n}(X_i - \overline{X})^2$.

We know that $\sum_{i=1}^{n}(X_i - \overline{X})^2 = \sum_{i=1}^{n}(X_i - \mu)^2 - n(\overline{X} - \mu)^2$.

Taking the expectation:

$$E[SS] = \sum_{i=1}^{n}E[(X_i - \mu)^2] - nE[(\overline{X} - \mu)^2]$$

Since $E[(X_i - \mu)^2] = \sigma^2$ and $E[(\overline{X} - \mu)^2] = Var(\overline{X}) = \sigma^2/n$:

$$E[SS] = n\sigma^2 - n(\frac{\sigma^2}{n}) = n\sigma^2 - \sigma^2 = (n-1)\sigma^2$$

**Proof of Unbiasedness (Task f):**

**For $\sigma^2_{n-1}$:**

$$E[\sigma^2_{n-1}] = E\left[\frac{1}{n-1} SS\right] = \frac{1}{n-1} (n-1)\sigma^2 = \sigma^2$$

Since $E[\sigma^2_{n-1}] = \sigma^2$, this estimator is **unbiased**.

**For $\sigma^2_{n}$:**

$$E[\sigma^2_{n}] = E\left[\frac{1}{n} SS\right] = \frac{1}{n} (n-1)\sigma^2 = \frac{n-1}{n}\sigma^2$$

Since $E[\sigma^2_{n}] \neq \sigma^2$, this estimator is **biased**.

### **Conclusion**

**Reliability and Common Sense (Tasks d & g):**

**Bias Analysis:** The simulation results confirm the theory. The bias for $\sigma^2_{n-1}$ is extremely close to 0 for all $n$ (random fluctuation). The bias for $\sigma^2_n$ is negative (approx $-0.4$ for $n=10$), meaning it underestimates the population variance.

**Effect of $n$:** The theoretical bias of $\sigma^2_n$ is $\sigma^2(\frac{n-1}{n} - 1) = -\frac{\sigma^2}{n}$. As $n$ increases (for example, to $n=1000$), this term approaches 0.

**Agreement:** Practical results agree with common sense expectations; for small sample sizes, dividing by $n$ instead of $n-1$ significantly underestimates the spread of the data because the sample mean is used instead of the true population mean, which minimizes the sum of squares.
